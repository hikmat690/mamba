{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmat690/mamba/blob/main/TurboMamba_TAP_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ TurboMamba-TAP: Weather-Robust Semantic Segmentation\n",
        "\n",
        "**Senior Deep Learning Engineer Implementation for Google Colab (T4 GPU)**\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture Overview:\n",
        "- **Stage 1:** TAP (Task-Adaptive Prompt) Cleaner - Removes weather degradation\n",
        "- **Stage 2:** Mamba-inspired Encoder - Long-range dependency modeling\n",
        "- **Stage 3:** Detail-preserving Decoder - High-quality segmentation\n",
        "\n",
        "## Dataset Structure:\n",
        "```\n",
        "dataset.zip\n",
        "‚îî‚îÄ‚îÄ acdc_night_train/\n",
        "    ‚îú‚îÄ‚îÄ folder_1/ (images)\n",
        "    ‚îú‚îÄ‚îÄ folder_2/ (images)\n",
        "    ‚îú‚îÄ‚îÄ folder_3/ (images)\n",
        "    ‚îú‚îÄ‚îÄ folder_4/ (images)\n",
        "    ‚îî‚îÄ‚îÄ folder_5/ (images)\n",
        "\n",
        "cityscapes.zip\n",
        "‚îî‚îÄ‚îÄ cityscapes_data/\n",
        "    ‚îú‚îÄ‚îÄ cityscapes_data/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ train/ (images)\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ val/ (images)\n",
        "    ‚îú‚îÄ‚îÄ train/ (images)\n",
        "    ‚îî‚îÄ‚îÄ val/ (images)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**üìå Instructions:**\n",
        "1. Upload `dataset.zip` and `cityscapes.zip` to `/content/`\n",
        "2. Run all cells in order\n",
        "3. Wait for training to complete (~20 epochs)\n",
        "4. Download your trained model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## üì¶ Step 1: Install Dependencies & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef163517-3f1e-4dd7-93ba-0697d82de075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All libraries imported successfully!\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (if needed)\n",
        "!pip install -q torch torchvision tqdm matplotlib pillow numpy\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import random\n",
        "import glob\n",
        "\n",
        "print(\"‚úì All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üìÇ Step 2: Extract & Verify Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "extract",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596d09e6-59f8-485d-b1cf-63adcb2c5aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TASK 1: Dataset Setup & Extraction\n",
            "======================================================================\n",
            "\n",
            "üì¶ Processing dataset.zip...\n",
            "   Extracting...\n",
            "   ‚úì Extracted\n",
            "   ‚úì Found at: /content/acdc_night_train\n",
            "\n",
            "üì¶ Processing cityscapes.zip...\n",
            "   Extracting...\n",
            "   ‚úì Extracted\n",
            "   ‚úì Found at: /content/cityscapes_data\n",
            "\n",
            "======================================================================\n",
            "Directory Structure Verification:\n",
            "======================================================================\n",
            "\n",
            "üìÅ acdc_night_train/\n",
            "  ‚îú‚îÄ‚îÄ GP020397/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (44 images)\n",
            "  ‚îú‚îÄ‚îÄ GP010376/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (56 images)\n",
            "  ‚îú‚îÄ‚îÄ GP010397/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (60 images)\n",
            "  ‚îú‚îÄ‚îÄ GOPR0376/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (147 images)\n",
            "  ‚îú‚îÄ‚îÄ GOPR0351/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (93 images)\n",
            "\n",
            "üìÅ cityscapes_data/\n",
            "  ‚îú‚îÄ‚îÄ val/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (500 images)\n",
            "  ‚îú‚îÄ‚îÄ train/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (2975 images)\n",
            "  ‚îú‚îÄ‚îÄ cityscapes_data/\n",
            "    ‚îú‚îÄ‚îÄ val/\n",
            "    ‚îÇ   ‚îî‚îÄ‚îÄ (500 images)\n",
            "    ‚îú‚îÄ‚îÄ train/\n",
            "    ‚îÇ   ‚îî‚îÄ‚îÄ (2975 images)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "‚úì Dataset extraction complete!\n",
            "\n",
            "Extracted datasets: ['dataset.zip', 'cityscapes.zip']\n"
          ]
        }
      ],
      "source": [
        "def setup_datasets():\n",
        "    \"\"\"\n",
        "    Extract datasets and verify structure.\n",
        "\n",
        "    Expected structure:\n",
        "    - dataset.zip ‚Üí acdc_night_train ‚Üí 5 folders with images\n",
        "    - cityscapes.zip ‚Üí cityscapes_data ‚Üí cityscapes_data/train, train, val\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"TASK 1: Dataset Setup & Extraction\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    base_path = '/content'\n",
        "\n",
        "    # Dataset configurations\n",
        "    datasets = {\n",
        "        'dataset.zip': 'acdc_night_train',\n",
        "        'cityscapes.zip': 'cityscapes_data'\n",
        "    }\n",
        "\n",
        "    extracted_paths = {}\n",
        "\n",
        "    for zip_file, expected_folder in datasets.items():\n",
        "        zip_path = os.path.join(base_path, zip_file)\n",
        "\n",
        "        # Check if zip file exists\n",
        "        if not os.path.exists(zip_path):\n",
        "            print(f\"‚ö†Ô∏è  WARNING: {zip_file} not found in /content/\")\n",
        "            print(f\"   Please upload {zip_file} to Colab before running this cell.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüì¶ Processing {zip_file}...\")\n",
        "\n",
        "        # Extract\n",
        "        print(f\"   Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(base_path)\n",
        "        print(f\"   ‚úì Extracted\")\n",
        "\n",
        "        # Find the actual extracted folder\n",
        "        # Sometimes zip files have the folder inside, sometimes they don't\n",
        "        possible_paths = [\n",
        "            os.path.join(base_path, expected_folder),\n",
        "            os.path.join(base_path, zip_file.replace('.zip', '')),\n",
        "        ]\n",
        "\n",
        "        extracted_path = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                extracted_path = path\n",
        "                break\n",
        "\n",
        "        if extracted_path:\n",
        "            extracted_paths[zip_file] = extracted_path\n",
        "            print(f\"   ‚úì Found at: {extracted_path}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not locate {expected_folder}\")\n",
        "\n",
        "    # Verify and print structure\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Directory Structure Verification:\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for zip_file, path in extracted_paths.items():\n",
        "        print(f\"\\nüìÅ {os.path.basename(path)}/\")\n",
        "\n",
        "        # Walk through directory structure\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            level = root.replace(path, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            folder_name = os.path.basename(root)\n",
        "\n",
        "            if level < 3:  # Only show up to 3 levels deep\n",
        "                if level > 0:\n",
        "                    print(f\"{indent}‚îú‚îÄ‚îÄ {folder_name}/\")\n",
        "\n",
        "                # Show image count\n",
        "                if files and level < 3:\n",
        "                    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "                    if image_files:\n",
        "                        print(f\"{indent}‚îÇ   ‚îî‚îÄ‚îÄ ({len(image_files)} images)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    return extracted_paths\n",
        "\n",
        "# Run extraction\n",
        "extracted_datasets = setup_datasets()\n",
        "\n",
        "print(\"\\n‚úì Dataset extraction complete!\")\n",
        "print(f\"\\nExtracted datasets: {list(extracted_datasets.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_class"
      },
      "source": [
        "## üîß Step 3: Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dataset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32cf82e3-4635-450a-d961-0b64621973e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dataset class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "class CombinedWeatherDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Multi-dataset loader for semantic segmentation.\n",
        "\n",
        "    Handles:\n",
        "    - ACDC Night Train: acdc_night_train/folder_X/images\n",
        "    - Cityscapes: cityscapes_data/train or val/images\n",
        "\n",
        "    Features:\n",
        "    - Automatic image/mask discovery\n",
        "    - RGB mask ‚Üí Class index conversion\n",
        "    - Unified 512x512 resizing for T4 GPU\n",
        "    - Handles images-only datasets (creates dummy masks)\n",
        "    \"\"\"\n",
        "\n",
        "    # Cityscapes 19-class color mapping\n",
        "    CITYSCAPES_COLORS = {\n",
        "        (128, 64, 128): 0,   # road\n",
        "        (244, 35, 232): 1,   # sidewalk\n",
        "        (70, 70, 70): 2,     # building\n",
        "        (102, 102, 156): 3,  # wall\n",
        "        (190, 153, 153): 4,  # fence\n",
        "        (153, 153, 153): 5,  # pole\n",
        "        (250, 170, 30): 6,   # traffic light\n",
        "        (220, 220, 0): 7,    # traffic sign\n",
        "        (107, 142, 35): 8,   # vegetation\n",
        "        (152, 251, 152): 9,  # terrain\n",
        "        (70, 130, 180): 10,  # sky\n",
        "        (220, 20, 60): 11,   # person\n",
        "        (255, 0, 0): 12,     # rider\n",
        "        (0, 0, 142): 13,     # car\n",
        "        (0, 0, 70): 14,      # truck\n",
        "        (0, 60, 100): 15,    # bus\n",
        "        (0, 80, 100): 16,    # train\n",
        "        (0, 0, 230): 17,     # motorcycle\n",
        "        (119, 11, 32): 18,   # bicycle\n",
        "    }\n",
        "\n",
        "    def __init__(self, root_dirs, img_size=512, has_masks=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dirs: List of root directories or single directory\n",
        "            img_size: Target size for resizing (default: 512x512)\n",
        "            has_masks: Whether dataset has ground truth masks\n",
        "        \"\"\"\n",
        "        self.root_dirs = root_dirs if isinstance(root_dirs, list) else [root_dirs]\n",
        "        self.img_size = img_size\n",
        "        self.has_masks = has_masks\n",
        "\n",
        "        # Find all images\n",
        "        self.samples = []\n",
        "        self._build_dataset()\n",
        "\n",
        "        print(f\"\\nüìä Dataset Statistics:\")\n",
        "        print(f\"   Total samples: {len(self.samples)}\")\n",
        "        print(f\"   Image size: {img_size}x{img_size}\")\n",
        "        print(f\"   Has masks: {has_masks}\")\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        \"\"\"Scan directories and build image list.\"\"\"\n",
        "        for root_dir in self.root_dirs:\n",
        "            root_path = Path(root_dir)\n",
        "\n",
        "            if not root_path.exists():\n",
        "                print(f\"‚ö†Ô∏è  Warning: {root_dir} does not exist, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Find all image files recursively\n",
        "            image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']\n",
        "\n",
        "            for ext in image_extensions:\n",
        "                # Search recursively for images\n",
        "                for img_path in root_path.rglob(ext):\n",
        "                    # Skip mask files (common naming patterns)\n",
        "                    if any(x in str(img_path).lower() for x in ['mask', 'label', 'gt', 'gtfine']):\n",
        "                        continue\n",
        "\n",
        "                    # Try to find corresponding mask\n",
        "                    mask_path = self._find_mask(img_path)\n",
        "\n",
        "                    self.samples.append({\n",
        "                        'image': str(img_path),\n",
        "                        'mask': str(mask_path) if mask_path else None\n",
        "                    })\n",
        "\n",
        "    def _find_mask(self, img_path):\n",
        "        \"\"\"Try to find corresponding mask file.\"\"\"\n",
        "        if not self.has_masks:\n",
        "            return None\n",
        "\n",
        "        img_path = Path(img_path)\n",
        "\n",
        "        # Common mask directory patterns\n",
        "        mask_patterns = [\n",
        "            img_path.parent.parent / 'masks' / img_path.name,\n",
        "            img_path.parent.parent / 'labels' / img_path.name,\n",
        "            img_path.parent.parent / 'gt' / img_path.name,\n",
        "            img_path.parent / 'masks' / img_path.name,\n",
        "            img_path.parent / 'labels' / img_path.name,\n",
        "        ]\n",
        "\n",
        "        for mask_path in mask_patterns:\n",
        "            if mask_path.exists():\n",
        "                return mask_path\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _rgb_to_class(self, mask_rgb):\n",
        "        \"\"\"Convert RGB mask to class indices.\"\"\"\n",
        "        mask_rgb = np.array(mask_rgb)\n",
        "        h, w = mask_rgb.shape[:2]\n",
        "        mask_class = np.zeros((h, w), dtype=np.int64)\n",
        "\n",
        "        # Convert RGB to class index\n",
        "        for color, class_idx in self.CITYSCAPES_COLORS.items():\n",
        "            matches = np.all(mask_rgb == color, axis=-1)\n",
        "            mask_class[matches] = class_idx\n",
        "\n",
        "        return mask_class\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(sample['image']).convert('RGB')\n",
        "        image = image.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "        image = np.array(image).astype(np.float32) / 255.0\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)  # HWC ‚Üí CHW\n",
        "\n",
        "        # Load or create mask\n",
        "        if sample['mask'] and os.path.exists(sample['mask']):\n",
        "            mask = Image.open(sample['mask'])\n",
        "            mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)\n",
        "\n",
        "            # Convert RGB mask to class indices if needed\n",
        "            if mask.mode == 'RGB':\n",
        "                mask = self._rgb_to_class(mask)\n",
        "            else:\n",
        "                mask = np.array(mask)\n",
        "\n",
        "            mask = torch.from_numpy(mask).long()\n",
        "        else:\n",
        "            # Create dummy mask (all zeros) if no mask available\n",
        "            mask = torch.zeros((self.img_size, self.img_size), dtype=torch.long)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "print(\"‚úì Dataset class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## üß† Step 4: TurboMamba-TAP Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tap_cleaner",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34bb065-2a78-45d7-f17b-9b45b349ca24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì TAP Cleaner defined!\n"
          ]
        }
      ],
      "source": [
        "class TAP_Cleaner(nn.Module):\n",
        "    \"\"\"\n",
        "    Task-Adaptive Prompt (TAP) Module\n",
        "    Removes weather degradation (night, fog, rain) using learnable prompts.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Learnable prompt tensor\n",
        "        self.prompt = nn.Parameter(torch.randn(1, in_channels, 1, 1) * 0.02)\n",
        "\n",
        "        # 3-layer Conv2d cleaner with residual\n",
        "        self.cleaner = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, 3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(hidden_dim, in_channels, 3, padding=1),\n",
        "            nn.Tanh()  # Residual in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add learnable prompt\n",
        "        x_prompted = x + self.prompt\n",
        "\n",
        "        # Generate residual correction\n",
        "        residual = self.cleaner(x_prompted)\n",
        "\n",
        "        # Apply residual (scaled for stability)\n",
        "        cleaned = x + 0.1 * residual\n",
        "        cleaned = torch.clamp(cleaned, 0, 1)\n",
        "\n",
        "        return cleaned, residual\n",
        "\n",
        "print(\"‚úì TAP Cleaner defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mamba_encoder",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19dda637-fd78-422e-d376-4debd265e3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Mamba Encoder defined!\n"
          ]
        }
      ],
      "source": [
        "class SimpleMambaEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Mamba-inspired Encoder (Pure PyTorch)\n",
        "    Uses Conv1d with large kernel to approximate selective scan.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_dim=128, num_layers=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Mamba blocks (Conv1d approximation of selective scan)\n",
        "        self.mamba_blocks = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.mamba_blocks.append(\n",
        "                nn.ModuleDict({\n",
        "                    'scan': nn.Sequential(\n",
        "                        nn.Conv1d(hidden_dim, hidden_dim, kernel_size=7,\n",
        "                                 padding=3, groups=hidden_dim),\n",
        "                        nn.GroupNorm(8, hidden_dim),\n",
        "                        nn.GELU(),\n",
        "                    ),\n",
        "                    'ffn': nn.Sequential(\n",
        "                        nn.Conv1d(hidden_dim, hidden_dim * 4, 1),\n",
        "                        nn.GELU(),\n",
        "                        nn.Conv1d(hidden_dim * 4, hidden_dim, 1),\n",
        "                    )\n",
        "                })\n",
        "            )\n",
        "\n",
        "        # Downsampling for multi-scale features\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim * 2, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, hidden_dim * 2),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial projection: B,3,512,512 ‚Üí B,128,256,256\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Flatten for 1D convolution (simulate sequence)\n",
        "        B, C, H, W = x.shape\n",
        "        x_flat = x.view(B, C, H * W)  # B,C,L where L=H*W\n",
        "\n",
        "        # Mamba blocks\n",
        "        for block in self.mamba_blocks:\n",
        "            # Selective scan\n",
        "            residual = x_flat\n",
        "            x_flat = block['scan'](x_flat) + residual\n",
        "\n",
        "            # FFN\n",
        "            residual = x_flat\n",
        "            x_flat = block['ffn'](x_flat) + residual\n",
        "\n",
        "        # Reshape back to 2D\n",
        "        x = x_flat.view(B, C, H, W)\n",
        "\n",
        "        # Downsample: B,128,256,256 ‚Üí B,256,128,128\n",
        "        x = self.downsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"‚úì Mamba Encoder defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "detail_head",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d30ff99-d267-4f1b-a5c9-1f0d3e24aab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Detail Head defined!\n"
          ]
        }
      ],
      "source": [
        "class DetailHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Detail-preserving Decoder\n",
        "    Progressive upsampling back to original resolution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=256, num_classes=19):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # 128x128 ‚Üí 256x256\n",
        "            nn.ConvTranspose2d(in_channels, 128, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 128),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # 256x256 ‚Üí 512x512\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # Final classification\n",
        "            nn.Conv2d(64, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "print(\"‚úì Detail Head defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "full_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35638df-d233-444d-9b6f-ca45343f811c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì TurboMamba-TAP model defined!\n"
          ]
        }
      ],
      "source": [
        "class TurboMambaTAP(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete TurboMamba-TAP Architecture\n",
        "\n",
        "    Pipeline:\n",
        "    Input ‚Üí TAP Cleaner ‚Üí Mamba Encoder ‚Üí Detail Head ‚Üí Segmentation\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=19):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tap_cleaner = TAP_Cleaner(in_channels=3, hidden_dim=64)\n",
        "        self.mamba_encoder = SimpleMambaEncoder(in_channels=3, hidden_dim=128, num_layers=4)\n",
        "        self.detail_head = DetailHead(in_channels=256, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, return_cleaned=False):\n",
        "        # Stage 1: TAP Cleaning\n",
        "        x_clean, residual = self.tap_cleaner(x)\n",
        "\n",
        "        # Stage 2: Mamba Encoding\n",
        "        features = self.mamba_encoder(x_clean)\n",
        "\n",
        "        # Stage 3: Detail Decoding\n",
        "        logits = self.detail_head(features)\n",
        "\n",
        "        if return_cleaned:\n",
        "            return logits, x_clean\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"‚úì TurboMamba-TAP model defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## üèãÔ∏è Step 5: Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "metrics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e044f6e-905d-49ea-a80f-347c0d6f9682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Metrics function defined!\n"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(pred, target, num_classes=19):\n",
        "    \"\"\"Calculate mIoU and pixel accuracy.\"\"\"\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    # Pixel accuracy\n",
        "    pixel_acc = (pred == target).mean()\n",
        "\n",
        "    # mIoU\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "\n",
        "        intersection = (pred_cls & target_cls).sum()\n",
        "        union = (pred_cls | target_cls).sum()\n",
        "\n",
        "        if union > 0:\n",
        "            ious.append(intersection / union)\n",
        "\n",
        "    miou = np.mean(ious) if ious else 0.0\n",
        "\n",
        "    return pixel_acc, miou\n",
        "\n",
        "print(\"‚úì Metrics function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "train_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f584773c-c32a-4c5c-deeb-c0bb94d41577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Training function defined!\n"
          ]
        }
      ],
      "source": [
        "def train(model, train_loader, val_loader, device, epochs=20, lr=1e-4):\n",
        "    \"\"\"\n",
        "    Training loop with progress tracking.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING TURBOMAMBA-TAP\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Batch size: {train_loader.batch_size}\")\n",
        "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "    print(f\"Epochs: {epochs}\")\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_miou': [],\n",
        "        'val_pixel_acc': []\n",
        "    }\n",
        "\n",
        "    best_miou = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ========== TRAINING ==========\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "        for images, masks in pbar:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # ========== VALIDATION ==========\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_pixel_acc = []\n",
        "        all_miou = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "            for images, masks in pbar:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, masks)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                pred = logits.argmax(dim=1)\n",
        "                pixel_acc, miou = calculate_metrics(pred, masks)\n",
        "                all_pixel_acc.append(pixel_acc)\n",
        "                all_miou.append(miou)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'mIoU': f\"{miou:.4f}\"\n",
        "                })\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        avg_pixel_acc = np.mean(all_pixel_acc)\n",
        "        avg_miou = np.mean(all_miou)\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_miou'].append(avg_miou)\n",
        "        history['val_pixel_acc'].append(avg_pixel_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "        print(f\"  Val mIoU:   {avg_miou:.4f}\")\n",
        "        print(f\"  Val Acc:    {avg_pixel_acc:.4f}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_miou > best_miou:\n",
        "            best_miou = avg_miou\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'miou': best_miou,\n",
        "            }, '/content/turbo_mamba_best.pth')\n",
        "            print(f\"‚úì Saved best model (mIoU: {best_miou:.4f})\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), '/content/turbo_mamba_colab.pth')\n",
        "    print(\"\\n‚úì Training complete! Model saved to: turbo_mamba_colab.pth\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"‚úì Training function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## üìä Step 6: Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "viz_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "833bee93-2c4f-4187-9905-4098b4bf591c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Visualization functions defined!\n"
          ]
        }
      ],
      "source": [
        "def visualize_results(model, val_dataset, device):\n",
        "    \"\"\"\n",
        "    Visualize: [Original Image, Cleaned, Ground Truth, Prediction]\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Generating Visualization...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get random sample\n",
        "    idx = random.randint(0, len(val_dataset) - 1)\n",
        "    image, mask = val_dataset[idx]\n",
        "\n",
        "    # Inference\n",
        "    image_input = image.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, cleaned = model(image_input, return_cleaned=True)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "    # Convert to numpy\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    cleaned_np = cleaned.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    mask_np = mask.cpu().numpy()\n",
        "    pred_np = pred.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Plot\n",
        "    cmap = plt.cm.get_cmap('tab20', 19)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    axes[0].imshow(image_np)\n",
        "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(cleaned_np)\n",
        "    axes[1].set_title('TAP Cleaned', fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(mask_np, cmap=cmap, vmin=0, vmax=18)\n",
        "    axes[2].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    im = axes[3].imshow(pred_np, cmap=cmap, vmin=0, vmax=18)\n",
        "    axes[3].set_title('Prediction', fontsize=14, fontweight='bold')\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.colorbar(im, ax=axes, orientation='horizontal', fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/turbo_mamba_result.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"‚úì Saved to: turbo_mamba_result.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training curves.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training & Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # mIoU\n",
        "    axes[1].plot(history['val_miou'], label='Val mIoU', marker='o', color='green')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('mIoU')\n",
        "    axes[1].set_title('Validation mIoU')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Pixel Accuracy\n",
        "    axes[2].plot(history['val_pixel_acc'], label='Val Pixel Acc', marker='o', color='orange')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Accuracy')\n",
        "    axes[2].set_title('Validation Pixel Accuracy')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/training_history.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"‚úì Saved to: training_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Visualization functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_execution"
      },
      "source": [
        "## üöÄ Step 7: Main Execution - Build Dataset & Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "create_datasets",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d4d4ac-b542-42c3-dbd3-0b57dcae0722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING COMBINED DATASET\n",
            "======================================================================\n",
            "Added: /content/acdc_night_train\n",
            "Added: /content/cityscapes_data\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "   Total samples: 7350\n",
            "   Image size: 512x512\n",
            "   Has masks: True\n",
            "\n",
            "‚úì Dataset split: 5880 train, 1470 val\n",
            "‚úì DataLoaders created!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING COMBINED DATASET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Collect all dataset paths\n",
        "all_data_dirs = []\n",
        "\n",
        "# Add extracted dataset paths\n",
        "for zip_file, path in extracted_datasets.items():\n",
        "    all_data_dirs.append(path)\n",
        "    print(f\"Added: {path}\")\n",
        "\n",
        "# Create combined dataset\n",
        "full_dataset = CombinedWeatherDataset(\n",
        "    root_dirs=all_data_dirs,\n",
        "    img_size=512,\n",
        "    has_masks=True  # Set to False if you don't have masks\n",
        ")\n",
        "\n",
        "# Split into train/val (80/20)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Dataset split: {train_size} train, {val_size} val\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,  # T4 GPU safe\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"‚úì DataLoaders created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "initialize_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "e589df23-0bcc-43c1-f4ad-22fbae71bcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "INITIALIZING TURBOMAMBA-TAP MODEL\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3062338609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nDevice: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INITIALIZING TURBOMAMBA-TAP MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nDevice: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = TurboMambaTAP(num_classes=19).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size: ~{total_params * 4 / 1024**2:.2f} MB\")\n",
        "print(f\"\\n‚úì Model ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_section"
      },
      "source": [
        "## üéØ Step 8: Train the Model\n",
        "\n",
        "**This will take approximately 30-60 minutes on T4 GPU for 20 epochs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "run_training",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8a71f30f-61f7-4cad-9245-9554b97f063c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3063958609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = train(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    epochs=20,\n",
        "    lr=1e-4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## üìà Step 9: Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_curves"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "visualize_results(model, val_dataset, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## üíæ Step 10: Download Your Trained Model\n",
        "\n",
        "Run the cell below to download your trained model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DOWNLOADING TRAINED MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Download final model\n",
        "if os.path.exists('/content/turbo_mamba_colab.pth'):\n",
        "    print(\"\\nDownloading turbo_mamba_colab.pth...\")\n",
        "    files.download('/content/turbo_mamba_colab.pth')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "# Download best model\n",
        "if os.path.exists('/content/turbo_mamba_best.pth'):\n",
        "    print(\"\\nDownloading turbo_mamba_best.pth...\")\n",
        "    files.download('/content/turbo_mamba_best.pth')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "# Download visualizations\n",
        "if os.path.exists('/content/turbo_mamba_result.png'):\n",
        "    print(\"\\nDownloading turbo_mamba_result.png...\")\n",
        "    files.download('/content/turbo_mamba_result.png')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "if os.path.exists('/content/training_history.png'):\n",
        "    print(\"\\nDownloading training_history.png...\")\n",
        "    files.download('/content/training_history.png')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úì ALL DOWNLOADS COMPLETE!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## üîÆ Bonus: Inference on New Images\n",
        "\n",
        "Use this cell to test your trained model on new images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_code"
      },
      "outputs": [],
      "source": [
        "def predict_single_image(model, image_path, device):\n",
        "    \"\"\"\n",
        "    Run inference on a single image.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize((512, 512), Image.BILINEAR)\n",
        "    image_np = np.array(image).astype(np.float32) / 255.0\n",
        "    image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        logits, cleaned = model(image_tensor, return_cleaned=True)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axes[0].imshow(image_np)\n",
        "    axes[0].set_title('Original', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    cleaned_np = cleaned.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    axes[1].imshow(cleaned_np)\n",
        "    axes[1].set_title('TAP Cleaned', fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    pred_np = pred.squeeze(0).cpu().numpy()\n",
        "    cmap = plt.cm.get_cmap('tab20', 19)\n",
        "    im = axes[2].imshow(pred_np, cmap=cmap, vmin=0, vmax=18)\n",
        "    axes[2].set_title('Segmentation', fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.colorbar(im, ax=axes, orientation='horizontal', fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (uncomment and provide your image path):\n",
        "# predict_single_image(model, '/content/your_test_image.jpg', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "---\n",
        "\n",
        "## üéâ Training Complete!\n",
        "\n",
        "### Generated Files:\n",
        "- ‚úÖ `turbo_mamba_colab.pth` - Final trained model\n",
        "- ‚úÖ `turbo_mamba_best.pth` - Best validation checkpoint\n",
        "- ‚úÖ `turbo_mamba_result.png` - Visualization\n",
        "- ‚úÖ `training_history.png` - Training curves\n",
        "\n",
        "### Model Architecture:\n",
        "- **TAP Cleaner**: Removes weather degradation\n",
        "- **Mamba Encoder**: Long-range feature extraction\n",
        "- **Detail Head**: High-resolution segmentation\n",
        "\n",
        "### Next Steps:\n",
        "1. Download your trained models\n",
        "2. Test on new images using the inference cell\n",
        "3. Fine-tune hyperparameters if needed\n",
        "4. Deploy to your application\n",
        "\n",
        "---\n",
        "\n",
        "**Need help?** Check the paper or reach out to the research team!\n",
        "\n",
        "**Senior Deep Learning Engineer** üöÄ"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}