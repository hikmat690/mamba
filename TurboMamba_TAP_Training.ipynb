{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmat690/mamba/blob/main/TurboMamba_TAP_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ TurboMamba-TAP: Weather-Robust Semantic Segmentation\n",
        "\n",
        "**Senior Deep Learning Engineer Implementation for Google Colab (T4 GPU)**\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture Overview:\n",
        "- **Stage 1:** TAP (Task-Adaptive Prompt) Cleaner - Removes weather degradation\n",
        "- **Stage 2:** Mamba-inspired Encoder - Long-range dependency modeling\n",
        "- **Stage 3:** Detail-preserving Decoder - High-quality segmentation\n",
        "\n",
        "## Dataset Structure:\n",
        "```\n",
        "dataset.zip\n",
        "‚îî‚îÄ‚îÄ acdc_night_train/\n",
        "    ‚îú‚îÄ‚îÄ folder_1/ (images)\n",
        "    ‚îú‚îÄ‚îÄ folder_2/ (images)\n",
        "    ‚îú‚îÄ‚îÄ folder_3/ (images)\n",
        "    ‚îú‚îÄ‚îÄ folder_4/ (images)\n",
        "    ‚îî‚îÄ‚îÄ folder_5/ (images)\n",
        "\n",
        "cityscapes.zip\n",
        "‚îî‚îÄ‚îÄ cityscapes_data/\n",
        "    ‚îú‚îÄ‚îÄ cityscapes_data/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ train/ (images)\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ val/ (images)\n",
        "    ‚îú‚îÄ‚îÄ train/ (images)\n",
        "    ‚îî‚îÄ‚îÄ val/ (images)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**üìå Instructions:**\n",
        "1. Upload `dataset.zip` and `cityscapes.zip` to `/content/`\n",
        "2. Run all cells in order\n",
        "3. Wait for training to complete (~20 epochs)\n",
        "4. Download your trained model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## üì¶ Step 1: Install Dependencies & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf558fc4-a263-4540-e70c-95f2974544b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All libraries imported successfully!\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (if needed)\n",
        "!pip install -q torch torchvision tqdm matplotlib pillow numpy\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import random\n",
        "import glob\n",
        "\n",
        "print(\"‚úì All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üìÇ Step 2: Extract & Verify Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "extract",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962376fd-2f46-46a8-b7a3-1f66c443fae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TASK 1: Dataset Setup & Extraction\n",
            "======================================================================\n",
            "\n",
            "üì¶ Processing dataset.zip...\n",
            "   Extracting...\n",
            "   ‚úì Extracted\n",
            "   ‚úì Found at: /content/acdc_night_train\n",
            "\n",
            "üì¶ Processing cityscapes.zip...\n",
            "   Extracting...\n",
            "   ‚úì Extracted\n",
            "   ‚úì Found at: /content/cityscapes_data\n",
            "\n",
            "======================================================================\n",
            "Directory Structure Verification:\n",
            "======================================================================\n",
            "\n",
            "üìÅ acdc_night_train/\n",
            "  ‚îú‚îÄ‚îÄ GP020397/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (44 images)\n",
            "  ‚îú‚îÄ‚îÄ GP010376/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (56 images)\n",
            "  ‚îú‚îÄ‚îÄ GP010397/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (60 images)\n",
            "  ‚îú‚îÄ‚îÄ GOPR0376/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (147 images)\n",
            "  ‚îú‚îÄ‚îÄ GOPR0351/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (93 images)\n",
            "\n",
            "üìÅ cityscapes_data/\n",
            "  ‚îú‚îÄ‚îÄ val/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (500 images)\n",
            "  ‚îú‚îÄ‚îÄ train/\n",
            "  ‚îÇ   ‚îî‚îÄ‚îÄ (2975 images)\n",
            "  ‚îú‚îÄ‚îÄ cityscapes_data/\n",
            "    ‚îú‚îÄ‚îÄ val/\n",
            "    ‚îÇ   ‚îî‚îÄ‚îÄ (500 images)\n",
            "    ‚îú‚îÄ‚îÄ train/\n",
            "    ‚îÇ   ‚îî‚îÄ‚îÄ (2975 images)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "‚úì Dataset extraction complete!\n",
            "\n",
            "Extracted datasets: ['dataset.zip', 'cityscapes.zip']\n"
          ]
        }
      ],
      "source": [
        "def setup_datasets():\n",
        "    \"\"\"\n",
        "    Extract datasets and verify structure.\n",
        "\n",
        "    Expected structure:\n",
        "    - dataset.zip ‚Üí acdc_night_train ‚Üí 5 folders with images\n",
        "    - cityscapes.zip ‚Üí cityscapes_data ‚Üí cityscapes_data/train, train, val\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"TASK 1: Dataset Setup & Extraction\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    base_path = '/content'\n",
        "\n",
        "    # Dataset configurations\n",
        "    datasets = {\n",
        "        'dataset.zip': 'acdc_night_train',\n",
        "        'cityscapes.zip': 'cityscapes_data'\n",
        "    }\n",
        "\n",
        "    extracted_paths = {}\n",
        "\n",
        "    for zip_file, expected_folder in datasets.items():\n",
        "        zip_path = os.path.join(base_path, zip_file)\n",
        "\n",
        "        # Check if zip file exists\n",
        "        if not os.path.exists(zip_path):\n",
        "            print(f\"‚ö†Ô∏è  WARNING: {zip_file} not found in /content/\")\n",
        "            print(f\"   Please upload {zip_file} to Colab before running this cell.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüì¶ Processing {zip_file}...\")\n",
        "\n",
        "        # Extract\n",
        "        print(f\"   Extracting...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(base_path)\n",
        "        print(f\"   ‚úì Extracted\")\n",
        "\n",
        "        # Find the actual extracted folder\n",
        "        # Sometimes zip files have the folder inside, sometimes they don't\n",
        "        possible_paths = [\n",
        "            os.path.join(base_path, expected_folder),\n",
        "            os.path.join(base_path, zip_file.replace('.zip', '')),\n",
        "        ]\n",
        "\n",
        "        extracted_path = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                extracted_path = path\n",
        "                break\n",
        "\n",
        "        if extracted_path:\n",
        "            extracted_paths[zip_file] = extracted_path\n",
        "            print(f\"   ‚úì Found at: {extracted_path}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not locate {expected_folder}\")\n",
        "\n",
        "    # Verify and print structure\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Directory Structure Verification:\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for zip_file, path in extracted_paths.items():\n",
        "        print(f\"\\nüìÅ {os.path.basename(path)}/\")\n",
        "\n",
        "        # Walk through directory structure\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            level = root.replace(path, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            folder_name = os.path.basename(root)\n",
        "\n",
        "            if level < 3:  # Only show up to 3 levels deep\n",
        "                if level > 0:\n",
        "                    print(f\"{indent}‚îú‚îÄ‚îÄ {folder_name}/\")\n",
        "\n",
        "                # Show image count\n",
        "                if files and level < 3:\n",
        "                    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "                    if image_files:\n",
        "                        print(f\"{indent}‚îÇ   ‚îî‚îÄ‚îÄ ({len(image_files)} images)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    return extracted_paths\n",
        "\n",
        "# Run extraction\n",
        "extracted_datasets = setup_datasets()\n",
        "\n",
        "print(\"\\n‚úì Dataset extraction complete!\")\n",
        "print(f\"\\nExtracted datasets: {list(extracted_datasets.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_class"
      },
      "source": [
        "## üîß Step 3: Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dataset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbcb174a-207c-420b-9f01-6c1390954772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Fixed Dataset class: Dummy mask set to 255 (Ignore)!\n"
          ]
        }
      ],
      "source": [
        "class CombinedWeatherDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Specially modified loader for:\n",
        "    1. Paired Cityscapes: (201MB version) Side-by-side Image | RGB Mask\n",
        "    2. Unpaired ACDC Night: Images only in acdc_night_train folders\n",
        "    \"\"\"\n",
        "\n",
        "    # Cityscapes 19-class color mapping\n",
        "    CITYSCAPES_COLORS = {\n",
        "        (128, 64, 128): 0, (244, 35, 232): 1, (70, 70, 70): 2, (102, 102, 156): 3,\n",
        "        (190, 153, 153): 4, (153, 153, 153): 5, (250, 170, 30): 6, (220, 220, 0): 7,\n",
        "        (107, 142, 35): 8, (152, 251, 152): 9, (70, 130, 180): 10, (220, 20, 60): 11,\n",
        "        (255, 0, 0): 12, (0, 0, 142): 13, (0, 0, 70): 14, (0, 60, 100): 15,\n",
        "        (0, 80, 100): 16, (0, 0, 230): 17, (119, 11, 32): 18\n",
        "    }\n",
        "\n",
        "    def __init__(self, root_dirs, img_size=512):\n",
        "        self.root_dirs = root_dirs if isinstance(root_dirs, list) else [root_dirs]\n",
        "        self.img_size = img_size\n",
        "        self.samples = []\n",
        "        self._build_dataset()\n",
        "\n",
        "        print(f\"\\nüìä Dataset Statistics:\")\n",
        "        print(f\"   Total samples: {len(self.samples)}\")\n",
        "        print(f\"   Image size: {img_size}x{img_size}\")\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        for root_dir in self.root_dirs:\n",
        "            root_path = Path(root_dir)\n",
        "            if not root_path.exists(): continue\n",
        "\n",
        "            files = list(root_path.rglob('*.png')) + list(root_path.rglob('*.jpg'))\n",
        "\n",
        "            for f in files:\n",
        "                # Identify Cityscapes vs ACDC\n",
        "                is_paired = 'cityscape' in str(f).lower()\n",
        "                self.samples.append({'path': str(f), 'is_paired': is_paired})\n",
        "\n",
        "    def _rgb_to_class(self, mask_rgb):\n",
        "        mask_rgb = np.array(mask_rgb)\n",
        "        h, w = mask_rgb.shape[:2]\n",
        "        mask_class = np.zeros((h, w), dtype=np.int64)\n",
        "        for color, class_idx in self.CITYSCAPES_COLORS.items():\n",
        "            matches = np.all(mask_rgb == color, axis=-1)\n",
        "            mask_class[matches] = class_idx\n",
        "        return mask_class\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        full_img = Image.open(sample['path']).convert('RGB')\n",
        "\n",
        "        if sample['is_paired']:\n",
        "            # PAIRED (Cityscapes): Split image\n",
        "            w, h = full_img.size\n",
        "            img = full_img.crop((0, 0, w//2, h))\n",
        "            mask_img = full_img.crop((w//2, 0, w, h))\n",
        "\n",
        "            img = img.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "            mask_img = mask_img.resize((self.img_size, self.img_size), Image.NEAREST)\n",
        "            mask = torch.from_numpy(self._rgb_to_class(mask_img)).long()\n",
        "        else:\n",
        "            # UNPAIRED (ACDC Night): Image only\n",
        "            img = full_img.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "\n",
        "            # --- THE FIX IS HERE ---\n",
        "            # Use 255 (Ignore) instead of 0 (Road)\n",
        "            mask = torch.full((self.img_size, self.img_size), 255, dtype=torch.long)\n",
        "            # -----------------------\n",
        "\n",
        "        img_np = np.array(img).astype(np.float32) / 255.0\n",
        "        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)\n",
        "\n",
        "        return img_tensor, mask\n",
        "\n",
        "print(\"‚úì Fixed Dataset class: Dummy mask set to 255 (Ignore)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## üß† Step 4: TurboMamba-TAP Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tap_cleaner",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef900cb4-a398-4339-e7b7-9429316974b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì TAP Cleaner defined!\n"
          ]
        }
      ],
      "source": [
        "class TAP_Cleaner(nn.Module):\n",
        "    \"\"\"\n",
        "    Task-Adaptive Prompt (TAP) Module\n",
        "    Removes weather degradation (night, fog, rain) using learnable prompts.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Learnable prompt tensor\n",
        "        self.prompt = nn.Parameter(torch.randn(1, in_channels, 1, 1) * 0.02)\n",
        "\n",
        "        # 3-layer Conv2d cleaner with residual\n",
        "        self.cleaner = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, 3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(hidden_dim, in_channels, 3, padding=1),\n",
        "            nn.Tanh()  # Residual in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add learnable prompt\n",
        "        x_prompted = x + self.prompt\n",
        "\n",
        "        # Generate residual correction\n",
        "        residual = self.cleaner(x_prompted)\n",
        "\n",
        "        # Apply residual (scaled for stability)\n",
        "        cleaned = x + 0.1 * residual\n",
        "        cleaned = torch.clamp(cleaned, 0, 1)\n",
        "\n",
        "        return cleaned, residual\n",
        "\n",
        "print(\"‚úì TAP Cleaner defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mamba_encoder",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae953216-975d-4145-f96e-2a4c4b71d9e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Mamba Encoder defined!\n"
          ]
        }
      ],
      "source": [
        "class SimpleMambaEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Mamba-inspired Encoder (Pure PyTorch)\n",
        "    Uses Conv1d with large kernel to approximate selective scan.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_dim=128, num_layers=4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Mamba blocks (Conv1d approximation of selective scan)\n",
        "        self.mamba_blocks = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.mamba_blocks.append(\n",
        "                nn.ModuleDict({\n",
        "                    'scan': nn.Sequential(\n",
        "                        nn.Conv1d(hidden_dim, hidden_dim, kernel_size=7,\n",
        "                                 padding=3, groups=hidden_dim),\n",
        "                        nn.GroupNorm(8, hidden_dim),\n",
        "                        nn.GELU(),\n",
        "                    ),\n",
        "                    'ffn': nn.Sequential(\n",
        "                        nn.Conv1d(hidden_dim, hidden_dim * 4, 1),\n",
        "                        nn.GELU(),\n",
        "                        nn.Conv1d(hidden_dim * 4, hidden_dim, 1),\n",
        "                    )\n",
        "                })\n",
        "            )\n",
        "\n",
        "        # Downsampling for multi-scale features\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim * 2, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, hidden_dim * 2),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial projection: B,3,512,512 ‚Üí B,128,256,256\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Flatten for 1D convolution (simulate sequence)\n",
        "        B, C, H, W = x.shape\n",
        "        x_flat = x.view(B, C, H * W)  # B,C,L where L=H*W\n",
        "\n",
        "        # Mamba blocks\n",
        "        for block in self.mamba_blocks:\n",
        "            # Selective scan\n",
        "            residual = x_flat\n",
        "            x_flat = block['scan'](x_flat) + residual\n",
        "\n",
        "            # FFN\n",
        "            residual = x_flat\n",
        "            x_flat = block['ffn'](x_flat) + residual\n",
        "\n",
        "        # Reshape back to 2D\n",
        "        x = x_flat.view(B, C, H, W)\n",
        "\n",
        "        # Downsample: B,128,256,256 ‚Üí B,256,128,128\n",
        "        x = self.downsample(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"‚úì Mamba Encoder defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "detail_head",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb73258e-ef10-4f20-fe2e-77de29073ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Detail Head defined!\n"
          ]
        }
      ],
      "source": [
        "class DetailHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Detail-preserving Decoder\n",
        "    Progressive upsampling back to original resolution.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=256, num_classes=19):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # 128x128 ‚Üí 256x256\n",
        "            nn.ConvTranspose2d(in_channels, 128, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 128),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # 256x256 ‚Üí 512x512\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # Final classification\n",
        "            nn.Conv2d(64, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "print(\"‚úì Detail Head defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "full_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b100b0e4-9a1c-46d5-f679-76939f035c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì TurboMamba-TAP model defined!\n"
          ]
        }
      ],
      "source": [
        "class TurboMambaTAP(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete TurboMamba-TAP Architecture\n",
        "\n",
        "    Pipeline:\n",
        "    Input ‚Üí TAP Cleaner ‚Üí Mamba Encoder ‚Üí Detail Head ‚Üí Segmentation\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=19):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tap_cleaner = TAP_Cleaner(in_channels=3, hidden_dim=64)\n",
        "        self.mamba_encoder = SimpleMambaEncoder(in_channels=3, hidden_dim=128, num_layers=4)\n",
        "        self.detail_head = DetailHead(in_channels=256, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, return_cleaned=False):\n",
        "        # Stage 1: TAP Cleaning\n",
        "        x_clean, residual = self.tap_cleaner(x)\n",
        "\n",
        "        # Stage 2: Mamba Encoding\n",
        "        features = self.mamba_encoder(x_clean)\n",
        "\n",
        "        # Stage 3: Detail Decoding\n",
        "        logits = self.detail_head(features)\n",
        "\n",
        "        if return_cleaned:\n",
        "            return logits, x_clean\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"‚úì TurboMamba-TAP model defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## üèãÔ∏è Step 5: Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "metrics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9879b4aa-a49d-4947-abea-994aaee5fbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Metrics function defined!\n"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(pred, target, num_classes=19):\n",
        "    \"\"\"Calculate mIoU and pixel accuracy.\"\"\"\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    # Pixel accuracy\n",
        "    pixel_acc = (pred == target).mean()\n",
        "\n",
        "    # mIoU\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "\n",
        "        intersection = (pred_cls & target_cls).sum()\n",
        "        union = (pred_cls | target_cls).sum()\n",
        "\n",
        "        if union > 0:\n",
        "            ious.append(intersection / union)\n",
        "\n",
        "    miou = np.mean(ious) if ious else 0.0\n",
        "\n",
        "    return pixel_acc, miou\n",
        "\n",
        "print(\"‚úì Metrics function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "train_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d537749a-efe2-4f37-86ec-4f1c67946fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Fixed Training Loop defined (with Gradient Clipping)!\n"
          ]
        }
      ],
      "source": [
        "def train(model, train_loader, val_loader, device, epochs=20, lr=5e-5): # <-- Lowered LR slightly\n",
        "    \"\"\"\n",
        "    Training loop with Gradient Clipping to fix 'nan' loss.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING TURBOMAMBA-TAP (STABLE VERSION)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # Training history\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_miou': [], 'val_pixel_acc': []}\n",
        "    best_miou = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ========== TRAINING ==========\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        valid_batches = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "        for images, masks in pbar:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, masks)\n",
        "\n",
        "            # --- CRITICAL FIX FOR 'NAN' ---\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"‚ö† Warning: Skipped batch with NaN loss\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients to prevent explosion (The Magic Fix)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        # Avoid division by zero if all batches fail\n",
        "        train_loss = train_loss / valid_batches if valid_batches > 0 else 0.0\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # ========== VALIDATION ==========\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_pixel_acc = []\n",
        "        all_miou = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, masks)\n",
        "                if not torch.isnan(loss):\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                pred = logits.argmax(dim=1)\n",
        "                pixel_acc, miou = calculate_metrics(pred, masks)\n",
        "                all_pixel_acc.append(pixel_acc)\n",
        "                all_miou.append(miou)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        avg_miou = np.mean(all_miou)\n",
        "        avg_pixel_acc = np.mean(all_pixel_acc)\n",
        "\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_miou'].append(avg_miou)\n",
        "        history['val_pixel_acc'].append(avg_pixel_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Summary: Loss: {train_loss:.4f} | mIoU: {avg_miou:.4f}\")\n",
        "\n",
        "        if avg_miou > best_miou:\n",
        "            best_miou = avg_miou\n",
        "            torch.save(model.state_dict(), '/content/turbo_mamba_best.pth')\n",
        "            print(f\"‚úì Saved Best: {best_miou:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"‚úì Fixed Training Loop defined (with Gradient Clipping)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## üìä Step 6: Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "viz_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3208c6dd-e8b3-4efc-8af8-e890a4221ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Visualization functions defined!\n"
          ]
        }
      ],
      "source": [
        "def visualize_results(model, val_dataset, device):\n",
        "    \"\"\"\n",
        "    Visualize: [Original Image, Cleaned, Ground Truth, Prediction]\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Generating Visualization...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get random sample\n",
        "    idx = random.randint(0, len(val_dataset) - 1)\n",
        "    image, mask = val_dataset[idx]\n",
        "\n",
        "    # Inference\n",
        "    image_input = image.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, cleaned = model(image_input, return_cleaned=True)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "    # Convert to numpy\n",
        "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    cleaned_np = cleaned.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    mask_np = mask.cpu().numpy()\n",
        "    pred_np = pred.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Plot\n",
        "    cmap = plt.cm.get_cmap('tab20', 19)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    axes[0].imshow(image_np)\n",
        "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(cleaned_np)\n",
        "    axes[1].set_title('TAP Cleaned', fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(mask_np, cmap=cmap, vmin=0, vmax=18)\n",
        "    axes[2].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    im = axes[3].imshow(pred_np, cmap=cmap, vmin=0, vmax=18)\n",
        "    axes[3].set_title('Prediction', fontsize=14, fontweight='bold')\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.colorbar(im, ax=axes, orientation='horizontal', fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/turbo_mamba_result.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"‚úì Saved to: turbo_mamba_result.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training curves.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training & Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # mIoU\n",
        "    axes[1].plot(history['val_miou'], label='Val mIoU', marker='o', color='green')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('mIoU')\n",
        "    axes[1].set_title('Validation mIoU')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Pixel Accuracy\n",
        "    axes[2].plot(history['val_pixel_acc'], label='Val Pixel Acc', marker='o', color='orange')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Accuracy')\n",
        "    axes[2].set_title('Validation Pixel Accuracy')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/training_history.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"‚úì Saved to: training_history.png\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Visualization functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_execution"
      },
      "source": [
        "## üöÄ Step 7: Main Execution - Build Dataset & Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "create_datasets",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81255fb-e4ad-4922-9b40-eaf9217ccabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING COMBINED DATASET\n",
            "======================================================================\n",
            "Added: /content/acdc_night_train\n",
            "Added: /content/cityscapes_data\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "   Total samples: 7350\n",
            "   Image size: 256x256\n",
            "\n",
            "‚úì Dataset split: 5880 train, 1470 val\n",
            "‚úì DataLoaders created!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING COMBINED DATASET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Collect all dataset paths\n",
        "all_data_dirs = []\n",
        "\n",
        "# Add extracted dataset paths\n",
        "for zip_file, path in extracted_datasets.items():\n",
        "    all_data_dirs.append(path)\n",
        "    print(f\"Added: {path}\")\n",
        "\n",
        "# Create combined dataset\n",
        "full_dataset = CombinedWeatherDataset(\n",
        "    root_dirs=all_data_dirs,\n",
        "    img_size=256 # Reduced from 512 to save GPU memory\n",
        ")\n",
        "\n",
        "# Split into train/val (80/20)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Dataset split: {train_size} train, {val_size} val\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,  # T4 GPU safe (will be re-adjusted in run_training if needed)\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"‚úì DataLoaders created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "initialize_model",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbde2b1-8363-407f-b931-b9aea7ae13e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "INITIALIZING TURBOMAMBA-TAP MODEL\n",
            "======================================================================\n",
            "\n",
            "Device: cpu\n",
            "\n",
            "üìä Model Statistics:\n",
            "   Total parameters: 1,529,369\n",
            "   Trainable parameters: 1,529,369\n",
            "   Model size: ~5.83 MB\n",
            "\n",
            "‚úì Model ready for training!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INITIALIZING TURBOMAMBA-TAP MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nDevice: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = TurboMambaTAP(num_classes=19).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size: ~{total_params * 4 / 1024**2:.2f} MB\")\n",
        "print(f\"\\n‚úì Model ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_section"
      },
      "source": [
        "## üéØ Step 8: Train the Model\n",
        "\n",
        "**This will take approximately 30-60 minutes on T4 GPU for 20 epochs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fa7fe5-e67b-4488-8401-007da88cc3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting to re-create DataLoaders with a smaller batch size (2) to resolve OutOfMemoryError...\n",
            "‚úì DataLoaders re-created with batch_size=2!\n",
            "\n",
            "======================================================================\n",
            "TRAINING TURBOMAMBA-TAP (STABLE VERSION)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20 [Train]:   0%|          | 0/2940 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/20 [Train]:   1%|          | 21/2940 [02:18<5:09:16,  6.36s/it, loss=1.5011]"
          ]
        }
      ],
      "source": [
        "# Re-create DataLoaders with a smaller batch size to mitigate OutOfMemoryError\n",
        "print(\"\\nAttempting to re-create DataLoaders with a smaller batch size (2) to resolve OutOfMemoryError...\")\n",
        "BATCH_SIZE = 2 # Reduced from 4\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "print(f\"‚úì DataLoaders re-created with batch_size={BATCH_SIZE}!\")\n",
        "\n",
        "# Train the model\n",
        "history = train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    epochs=20,\n",
        "    lr=1e-4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## üìà Step 9: Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_curves"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "visualize_results(model, val_dataset, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## üíæ Step 10: Download Your Trained Model\n",
        "\n",
        "Run the cell below to download your trained model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DOWNLOADING TRAINED MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Download final model\n",
        "if os.path.exists('/content/turbo_mamba_colab.pth'):\n",
        "    print(\"\\nDownloading turbo_mamba_colab.pth...\")\n",
        "    files.download('/content/turbo_mamba_colab.pth')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "# Download best model\n",
        "if os.path.exists('/content/turbo_mamba_best.pth'):\n",
        "    print(\"\\nDownloading turbo_mamba_best.pth...\")\n",
        "    files.download('/content/turbo_mamba_best.pth')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "# Download visualizations\n",
        "if os.path.exists('/content/turbo_mamba_result.png'):\n",
        "    print(\"\\nDownloading turbo_mamba_result.png...\")\n",
        "    files.download('/content/turbo_mamba_result.png')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "if os.path.exists('/content/training_history.png'):\n",
        "    print(\"\\nDownloading training_history.png...\")\n",
        "    files.download('/content/training_history.png')\n",
        "    print(\"‚úì Downloaded!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úì ALL DOWNLOADS COMPLETE!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## üîÆ Bonus: Inference on New Images\n",
        "\n",
        "Use this cell to test your trained model on new images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_code"
      },
      "outputs": [],
      "source": [
        "def predict_single_image(model, image_path, device):\n",
        "    \"\"\"\n",
        "    Run inference on a single image.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize((512, 512), Image.BILINEAR)\n",
        "    image_np = np.array(image).astype(np.float32) / 255.0\n",
        "    image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        logits, cleaned = model(image_tensor, return_cleaned=True)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axes[0].imshow(image_np)\n",
        "    axes[0].set_title('Original', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    cleaned_np = cleaned.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "    axes[1].imshow(cleaned_np)\n",
        "    axes[1].set_title('TAP Cleaned', fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    pred_np = pred.squeeze(0).cpu().numpy()\n",
        "    cmap = plt.cm.get_cmap('tab20', 19)\n",
        "    im = axes[2].imshow(pred_np, cmap=cmap, vmin=0, vmax=18)\n",
        "    axes[2].set_title('Segmentation', fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.colorbar(im, ax=axes, orientation='horizontal', fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (uncomment and provide your image path):\n",
        "# predict_single_image(model, '/content/your_test_image.jpg', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "---\n",
        "\n",
        "## üéâ Training Complete!\n",
        "\n",
        "### Generated Files:\n",
        "- ‚úÖ `turbo_mamba_colab.pth` - Final trained model\n",
        "- ‚úÖ `turbo_mamba_best.pth` - Best validation checkpoint\n",
        "- ‚úÖ `turbo_mamba_result.png` - Visualization\n",
        "- ‚úÖ `training_history.png` - Training curves\n",
        "\n",
        "### Model Architecture:\n",
        "- **TAP Cleaner**: Removes weather degradation\n",
        "- **Mamba Encoder**: Long-range feature extraction\n",
        "- **Detail Head**: High-resolution segmentation\n",
        "\n",
        "### Next Steps:\n",
        "1. Download your trained models\n",
        "2. Test on new images using the inference cell\n",
        "3. Fine-tune hyperparameters if needed\n",
        "4. Deploy to your application\n",
        "\n",
        "---\n",
        "\n",
        "**Need help?** Check the paper or reach out to the research team!\n",
        "\n",
        "**Senior Deep Learning Engineer** üöÄ"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}